{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1RaGvtPW7JIpulW9mIyKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmd03/DL_study/blob/main/Leaf_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijuEaK4UeNyw",
        "outputId": "fad5bedc-6dcc-4ee2-bee8-fda02d456139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -qq '/content/drive/MyDrive/dataset (1).zip' -d './dataset'"
      ],
      "metadata": {
        "id": "280HdlJ_eVRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data ready ##\n",
        "import os\n",
        "\n",
        "original_dataset_dir = './dataset'\n",
        "classes_list = os.listdir(original_dataset_dir)\n",
        "base_dir = './splitted'\n",
        "os.mkdir(base_dir)"
      ],
      "metadata": {
        "id": "TFVtRpT_gqjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  shutil\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list:\n",
        "  os.mkdir(os.path.join(train_dir, cls))\n",
        "  os.mkdir(os.path.join(validation_dir, cls))\n",
        "  os.mkdir(os.path.join(test_dir, cls))"
      ],
      "metadata": {
        "id": "B--zsw5shFPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        "\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "\n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm7ZLJvShev8",
        "outputId": "0c788f67-54b1-4a6d-e319-bd7bcc9aca3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
            "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Train size( Tomato___Early_blight ):  600\n",
            "Validation size( Tomato___Early_blight ):  200\n",
            "Test size( Tomato___Early_blight ):  200\n",
            "Train size( Tomato___Septoria_leaf_spot ):  1062\n",
            "Validation size( Tomato___Septoria_leaf_spot ):  354\n",
            "Test size( Tomato___Septoria_leaf_spot ):  354\n",
            "Train size( Tomato___Tomato_mosaic_virus ):  223\n",
            "Validation size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Test size( Tomato___Tomato_mosaic_virus ):  74\n",
            "Train size( Potato___Early_blight ):  600\n",
            "Validation size( Potato___Early_blight ):  200\n",
            "Test size( Potato___Early_blight ):  200\n",
            "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
            "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Train size( Tomato___healthy ):  954\n",
            "Validation size( Tomato___healthy ):  318\n",
            "Test size( Tomato___healthy ):  318\n",
            "Train size( Apple___Cedar_apple_rust ):  165\n",
            "Validation size( Apple___Cedar_apple_rust ):  55\n",
            "Test size( Apple___Cedar_apple_rust ):  55\n",
            "Train size( Grape___Black_rot ):  708\n",
            "Validation size( Grape___Black_rot ):  236\n",
            "Test size( Grape___Black_rot ):  236\n",
            "Train size( Tomato___Bacterial_spot ):  1276\n",
            "Validation size( Tomato___Bacterial_spot ):  425\n",
            "Test size( Tomato___Bacterial_spot ):  425\n",
            "Train size( Apple___healthy ):  987\n",
            "Validation size( Apple___healthy ):  329\n",
            "Test size( Apple___healthy ):  329\n",
            "Train size( Apple___Apple_scab ):  378\n",
            "Validation size( Apple___Apple_scab ):  126\n",
            "Test size( Apple___Apple_scab ):  126\n",
            "Train size( Grape___healthy ):  253\n",
            "Validation size( Grape___healthy ):  84\n",
            "Test size( Grape___healthy ):  84\n",
            "Train size( Cherry___Powdery_mildew ):  631\n",
            "Validation size( Cherry___Powdery_mildew ):  210\n",
            "Test size( Cherry___Powdery_mildew ):  210\n",
            "Train size( Tomato___Spider_mites Two-spotted_spider_mite ):  1005\n",
            "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Test size( Tomato___Spider_mites Two-spotted_spider_mite ):  335\n",
            "Train size( Potato___healthy ):  91\n",
            "Validation size( Potato___healthy ):  30\n",
            "Test size( Potato___healthy ):  30\n",
            "Train size( Corn___Common_rust ):  715\n",
            "Validation size( Corn___Common_rust ):  238\n",
            "Test size( Corn___Common_rust ):  238\n",
            "Train size( Grape___Esca_(Black_Measles) ):  829\n",
            "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
            "Test size( Grape___Esca_(Black_Measles) ):  276\n",
            "Train size( Corn___Northern_Leaf_Blight ):  591\n",
            "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
            "Test size( Corn___Northern_Leaf_Blight ):  197\n",
            "Train size( Strawberry___healthy ):  273\n",
            "Validation size( Strawberry___healthy ):  91\n",
            "Test size( Strawberry___healthy ):  91\n",
            "Train size( Pepper,_bell___healthy ):  886\n",
            "Validation size( Pepper,_bell___healthy ):  295\n",
            "Test size( Pepper,_bell___healthy ):  295\n",
            "Train size( Peach___healthy ):  216\n",
            "Validation size( Peach___healthy ):  72\n",
            "Test size( Peach___healthy ):  72\n",
            "Train size( Tomato___Target_Spot ):  842\n",
            "Validation size( Tomato___Target_Spot ):  280\n",
            "Test size( Tomato___Target_Spot ):  280\n",
            "Train size( Tomato___Late_blight ):  1145\n",
            "Validation size( Tomato___Late_blight ):  381\n",
            "Test size( Tomato___Late_blight ):  381\n",
            "Train size( Tomato___Leaf_Mold ):  571\n",
            "Validation size( Tomato___Leaf_Mold ):  190\n",
            "Test size( Tomato___Leaf_Mold ):  190\n",
            "Train size( Strawberry___Leaf_scorch ):  665\n",
            "Validation size( Strawberry___Leaf_scorch ):  221\n",
            "Test size( Strawberry___Leaf_scorch ):  221\n",
            "Train size( Cherry___healthy ):  512\n",
            "Validation size( Cherry___healthy ):  170\n",
            "Test size( Cherry___healthy ):  170\n",
            "Train size( Potato___Late_blight ):  600\n",
            "Validation size( Potato___Late_blight ):  200\n",
            "Test size( Potato___Late_blight ):  200\n",
            "Train size( Apple___Black_rot ):  372\n",
            "Validation size( Apple___Black_rot ):  124\n",
            "Test size( Apple___Black_rot ):  124\n",
            "Train size( Corn___healthy ):  697\n",
            "Validation size( Corn___healthy ):  232\n",
            "Test size( Corn___healthy ):  232\n",
            "Train size( Peach___Bacterial_spot ):  1378\n",
            "Validation size( Peach___Bacterial_spot ):  459\n",
            "Test size( Peach___Bacterial_spot ):  459\n",
            "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
            "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  3214\n",
            "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n",
            "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ):  1071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ready for training ##\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "a5iYN40kil5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ],
      "metadata": {
        "id": "OH11x4hCi4c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle=True, num_workers=4)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gwMSs0Mj_fd",
        "outputId": "96816c0c-1599-4e9e-d4d6-cdefd3dd2ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512)\n",
        "        self.fc2 = nn.Linear(512, 33)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = x.view(-1, 4096)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model_base = Net().to(device)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "K8FeRmyckVoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "lAX4qJCvltUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        since = time.time()\n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('-------------- epoch {} --------------'.format(epoch))\n",
        "        print('train loss : {:4f}, train acc : {:2f}'.format(train_loss, train_acc))\n",
        "        print('val loss : {:4f}, val acc : {:2f}'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, epochs)\n",
        "torch.save(base,'baseline.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL-6IAcfmWK9",
        "outputId": "f1d40f16-d702-4b57-db5e-f2fff927b6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 --------------\n",
            "train loss : 1.747927, train acc : 51.248489\n",
            "val loss : 1.787474, val acc : 50.231568\n",
            "Completed in 1m 8s\n",
            "-------------- epoch 2 --------------\n",
            "train loss : 1.105430, train acc : 66.051107\n",
            "val loss : 1.160378, val acc : 64.376017\n",
            "Completed in 1m 6s\n",
            "-------------- epoch 3 --------------\n",
            "train loss : 0.818230, train acc : 75.338697\n",
            "val loss : 0.876839, val acc : 73.563650\n",
            "Completed in 1m 8s\n",
            "-------------- epoch 4 --------------\n",
            "train loss : 0.629337, train acc : 80.399350\n",
            "val loss : 0.692149, val acc : 78.220053\n",
            "Completed in 1m 11s\n",
            "-------------- epoch 5 --------------\n",
            "train loss : 0.523273, train acc : 83.792572\n",
            "val loss : 0.587022, val acc : 81.549631\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 6 --------------\n",
            "train loss : 0.430138, train acc : 86.635541\n",
            "val loss : 0.500912, val acc : 84.203280\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 7 --------------\n",
            "train loss : 0.382040, train acc : 87.848597\n",
            "val loss : 0.464119, val acc : 85.079484\n",
            "Completed in 1m 6s\n",
            "-------------- epoch 8 --------------\n",
            "train loss : 0.362638, train acc : 88.811539\n",
            "val loss : 0.449566, val acc : 85.705345\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 9 --------------\n",
            "train loss : 0.334691, train acc : 89.532702\n",
            "val loss : 0.419840, val acc : 86.594067\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 10 --------------\n",
            "train loss : 0.284494, train acc : 90.858310\n",
            "val loss : 0.386793, val acc : 87.307548\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 11 --------------\n",
            "train loss : 0.286346, train acc : 91.033390\n",
            "val loss : 0.387844, val acc : 87.295031\n",
            "Completed in 1m 6s\n",
            "-------------- epoch 12 --------------\n",
            "train loss : 0.271418, train acc : 91.396057\n",
            "val loss : 0.383266, val acc : 87.395168\n",
            "Completed in 1m 6s\n",
            "-------------- epoch 13 --------------\n",
            "train loss : 0.197025, train acc : 94.184835\n",
            "val loss : 0.311502, val acc : 90.011265\n",
            "Completed in 1m 4s\n",
            "-------------- epoch 14 --------------\n",
            "train loss : 0.202207, train acc : 93.755471\n",
            "val loss : 0.325083, val acc : 89.447991\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 15 --------------\n",
            "train loss : 0.159889, train acc : 95.377048\n",
            "val loss : 0.276757, val acc : 91.187883\n",
            "Completed in 1m 6s\n",
            "-------------- epoch 16 --------------\n",
            "train loss : 0.169923, train acc : 94.981033\n",
            "val loss : 0.294324, val acc : 90.687195\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 17 --------------\n",
            "train loss : 0.138576, train acc : 96.019009\n",
            "val loss : 0.265405, val acc : 91.513331\n",
            "Completed in 1m 10s\n",
            "-------------- epoch 18 --------------\n",
            "train loss : 0.143412, train acc : 95.714703\n",
            "val loss : 0.275223, val acc : 90.950056\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 19 --------------\n",
            "train loss : 0.127235, train acc : 96.094043\n",
            "val loss : 0.265147, val acc : 91.688572\n",
            "Completed in 1m 9s\n",
            "-------------- epoch 20 --------------\n",
            "train loss : 0.232691, train acc : 91.925466\n",
            "val loss : 0.404097, val acc : 86.806859\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 21 --------------\n",
            "train loss : 0.115743, train acc : 96.602610\n",
            "val loss : 0.252219, val acc : 91.951433\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 22 --------------\n",
            "train loss : 0.098166, train acc : 97.140356\n",
            "val loss : 0.249531, val acc : 91.876330\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 23 --------------\n",
            "train loss : 0.097856, train acc : 97.227896\n",
            "val loss : 0.240155, val acc : 91.913882\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 24 --------------\n",
            "train loss : 0.093459, train acc : 97.248739\n",
            "val loss : 0.240893, val acc : 92.452122\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 25 --------------\n",
            "train loss : 0.079978, train acc : 97.824003\n",
            "val loss : 0.218924, val acc : 93.002879\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 26 --------------\n",
            "train loss : 0.072875, train acc : 98.161657\n",
            "val loss : 0.221825, val acc : 92.927776\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 27 --------------\n",
            "train loss : 0.074850, train acc : 97.949060\n",
            "val loss : 0.233080, val acc : 92.727500\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 28 --------------\n",
            "train loss : 0.057871, train acc : 98.636875\n",
            "val loss : 0.210620, val acc : 93.453499\n",
            "Completed in 1m 7s\n",
            "-------------- epoch 29 --------------\n",
            "train loss : 0.057060, train acc : 98.736921\n",
            "val loss : 0.198525, val acc : 93.603705\n",
            "Completed in 1m 5s\n",
            "-------------- epoch 30 --------------\n",
            "train loss : 0.063074, train acc : 98.340906\n",
            "val loss : 0.226869, val acc : 92.702466\n",
            "Completed in 1m 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize([64,64]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomCrop(52),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
        "\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize([64,64]),\n",
        "        transforms.RandomCrop(52),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "metadata": {
        "id": "lPrX3Yrvm4sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './splitted'\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x),\n",
        "                                  transform=data_transforms[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzfg-w2EYgln",
        "outputId": "c7f22747-921f-47f4-b1ef-b7188102e729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet = models.resnet50(pretrained=True) #pretrained=True : 학습 완료된 weight도 가져옴\n",
        "num_ftrs = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_ftrs, 33)\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG6MM5fzY9OJ",
        "outputId": "b7108a23-f96c-4eb6-abc5-5079500fea0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 141MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ct = 0\n",
        "\n",
        "for child in resnet.children():\n",
        "    ct += 1\n",
        "    if ct < 6:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} --------------'.format(epoch+1))\n",
        "        since = time.time()\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train' :\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Hi22H7feaG2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=epochs)\n",
        "torch.save(model_resnet50, 'resnet50.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0IK2r7Sb_BS",
        "outputId": "b58b4c32-3dae-4d65-bdd3-0e1699de038b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 --------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5768 Acc: 0.8232\n",
            "val Loss: 0.2768 Acc: 0.9099\n",
            "Completed in 0m 52s\n",
            "-------------- epoch 2 --------------\n",
            "train Loss: 0.2351 Acc: 0.9259\n",
            "val Loss: 0.2103 Acc: 0.9344\n",
            "Completed in 0m 52s\n",
            "-------------- epoch 3 --------------\n",
            "train Loss: 0.1596 Acc: 0.9501\n",
            "val Loss: 0.1816 Acc: 0.9412\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 4 --------------\n",
            "train Loss: 0.1427 Acc: 0.9531\n",
            "val Loss: 0.2535 Acc: 0.9249\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 5 --------------\n",
            "train Loss: 0.1135 Acc: 0.9634\n",
            "val Loss: 0.1336 Acc: 0.9588\n",
            "Completed in 0m 53s\n",
            "-------------- epoch 6 --------------\n",
            "train Loss: 0.1037 Acc: 0.9665\n",
            "val Loss: 0.1346 Acc: 0.9584\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 7 --------------\n",
            "train Loss: 0.0936 Acc: 0.9694\n",
            "val Loss: 0.1212 Acc: 0.9586\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 8 --------------\n",
            "train Loss: 0.0493 Acc: 0.9837\n",
            "val Loss: 0.0465 Acc: 0.9847\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 9 --------------\n",
            "train Loss: 0.0305 Acc: 0.9901\n",
            "val Loss: 0.0408 Acc: 0.9871\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 10 --------------\n",
            "train Loss: 0.0259 Acc: 0.9916\n",
            "val Loss: 0.0357 Acc: 0.9884\n",
            "Completed in 0m 52s\n",
            "-------------- epoch 11 --------------\n",
            "train Loss: 0.0218 Acc: 0.9931\n",
            "val Loss: 0.0363 Acc: 0.9880\n",
            "Completed in 0m 55s\n",
            "-------------- epoch 12 --------------\n",
            "train Loss: 0.0217 Acc: 0.9930\n",
            "val Loss: 0.0323 Acc: 0.9900\n",
            "Completed in 0m 56s\n",
            "-------------- epoch 13 --------------\n",
            "train Loss: 0.0201 Acc: 0.9931\n",
            "val Loss: 0.0336 Acc: 0.9890\n",
            "Completed in 0m 55s\n",
            "-------------- epoch 14 --------------\n",
            "train Loss: 0.0172 Acc: 0.9942\n",
            "val Loss: 0.0323 Acc: 0.9899\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 15 --------------\n",
            "train Loss: 0.0162 Acc: 0.9947\n",
            "val Loss: 0.0295 Acc: 0.9901\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 16 --------------\n",
            "train Loss: 0.0148 Acc: 0.9951\n",
            "val Loss: 0.0290 Acc: 0.9914\n",
            "Completed in 0m 53s\n",
            "-------------- epoch 17 --------------\n",
            "train Loss: 0.0153 Acc: 0.9948\n",
            "val Loss: 0.0323 Acc: 0.9897\n",
            "Completed in 0m 55s\n",
            "-------------- epoch 18 --------------\n",
            "train Loss: 0.0142 Acc: 0.9955\n",
            "val Loss: 0.0306 Acc: 0.9895\n",
            "Completed in 0m 52s\n",
            "-------------- epoch 19 --------------\n",
            "train Loss: 0.0125 Acc: 0.9961\n",
            "val Loss: 0.0285 Acc: 0.9910\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 20 --------------\n",
            "train Loss: 0.0165 Acc: 0.9950\n",
            "val Loss: 0.0266 Acc: 0.9915\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 21 --------------\n",
            "train Loss: 0.0133 Acc: 0.9964\n",
            "val Loss: 0.0273 Acc: 0.9915\n",
            "Completed in 0m 53s\n",
            "-------------- epoch 22 --------------\n",
            "train Loss: 0.0135 Acc: 0.9955\n",
            "val Loss: 0.0292 Acc: 0.9911\n",
            "Completed in 0m 53s\n",
            "-------------- epoch 23 --------------\n",
            "train Loss: 0.0119 Acc: 0.9967\n",
            "val Loss: 0.0268 Acc: 0.9912\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 24 --------------\n",
            "train Loss: 0.0131 Acc: 0.9958\n",
            "val Loss: 0.0314 Acc: 0.9906\n",
            "Completed in 0m 54s\n",
            "-------------- epoch 25 --------------\n",
            "train Loss: 0.0130 Acc: 0.9957\n",
            "val Loss: 0.0317 Acc: 0.9904\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 26 --------------\n",
            "train Loss: 0.0128 Acc: 0.9958\n",
            "val Loss: 0.0306 Acc: 0.9916\n",
            "Completed in 0m 53s\n",
            "-------------- epoch 27 --------------\n",
            "train Loss: 0.0125 Acc: 0.9960\n",
            "val Loss: 0.0302 Acc: 0.9900\n",
            "Completed in 0m 51s\n",
            "-------------- epoch 28 --------------\n",
            "train Loss: 0.0130 Acc: 0.9957\n",
            "val Loss: 0.0308 Acc: 0.9904\n",
            "Completed in 0m 52s\n",
            "-------------- epoch 29 --------------\n",
            "train Loss: 0.0144 Acc: 0.9959\n",
            "val Loss: 0.0275 Acc: 0.9907\n",
            "Completed in 0m 54s\n",
            "-------------- epoch 30 --------------\n",
            "train Loss: 0.0122 Acc: 0.9961\n",
            "val Loss: 0.0285 Acc: 0.9905\n",
            "Completed in 0m 53s\n",
            "Best val Acc: 0.991613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_resNet = transforms.Compose([\n",
        "    transforms.Resize([64,64]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet)\n",
        "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "resnet50 = torch.load('resnet50.pt')\n",
        "resnet50.eval()\n",
        "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
        "print('ResNet test acc :', test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG2-SNR9cVQH",
        "outputId": "6fb167cb-1e5f-4190-fb36-0970d0a1d8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet test acc : 98.73576167229942\n"
          ]
        }
      ]
    }
  ]
}